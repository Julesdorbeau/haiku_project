{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data scrapping complete notebook\n",
    "\n",
    "This notebook compile the different codes and functions used to do the web scrapping that I did to get all the haikus data that I used in this project. \n",
    "\n",
    "This is an important part for me as I never did web scrapping before this project and this was the opportunity to discover and learn some new very interesting things !\n",
    "\n",
    "This is mainly the part of the project that could easily be improved as having more and more experience in this new domain for me would be benefic and having more data would help my models to learn better and give better predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "from langdetect import detect\n",
    "import pandas as pd\n",
    "import requests\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Herons nest\n",
    "\n",
    "In this section, we are scrapping data from the Herons Nest website.\n",
    "\n",
    "This step parsing each archives pages of the site and collect the found haikus to store them in a list before saving them in a csv file. \n",
    "\n",
    "This website was found in the list of this GitHub during my research about the subject : [haiku-scraper's GitHub](https://github.com/ytixu/haiku-scraper/tree/master).\n",
    "\n",
    "This is the website used here : [Herons Nest](https://theheronsnest.com/June2024/index.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parsing_archive_page(archive_page_link) :\n",
    "    page_index = 1\n",
    "\n",
    "    # List to store all the haikus \n",
    "    archive_haikus_list = []\n",
    "\n",
    "    # Loop to get all the possible pages\n",
    "    archive_parsing = True\n",
    "    while archive_parsing :\n",
    "        # Getting the final link and doing the request\n",
    "        archive_link = archive_page_link + str(page_index) + \".html\"\n",
    "        # print(f\"Parsing page {archive_link}...\")\n",
    "        response_archive = requests.get(archive_link, headers={\"User-Agent\": \"XY\"})\n",
    "\n",
    "        # Checking the status of the request (if we got a 404 or not) and the link of the request (if we got redirected)\n",
    "        response_status = response_archive.status_code\n",
    "        response_link = response_archive.request.url\n",
    "        \n",
    "        # If we have a successful request, then we are parsing it\n",
    "        if response_status != 404 and (response_link == archive_link) :\n",
    "            archive_soup = bs4.BeautifulSoup(response_archive.text)\n",
    "\n",
    "            # Parsing the texts\n",
    "            haikus_html_list = archive_soup.body.find_all('p', {'class':\"haiku\"})\n",
    "\n",
    "            # Now getting the corps\n",
    "            for haiku in haikus_html_list :\n",
    "                haiku_corps = haiku.text.split('\\n')\n",
    "                if len(haiku_corps) == 3 :\n",
    "                    haiku_corps = [re.sub(r'\\xa0', '', haiku_text) for haiku_text in haiku_corps]\n",
    "                    haiku_corps = [haiku_text.strip() for haiku_text in haiku_corps]\n",
    "                    archive_haikus_list.append(haiku_corps)\n",
    "        else :\n",
    "            archive_parsing = False\n",
    "        \n",
    "        page_index += 1\n",
    "    return archive_haikus_list\n",
    "\n",
    "\n",
    "def parse_heronsnest_haikus(verbose = False) :\n",
    "    response = requests.get(\"https://theheronsnest.com/archives.html\", headers={\"User-Agent\": \"XY\"})\n",
    "    soup = bs4.BeautifulSoup(response.text)\n",
    "    archives_href = [i['href'] for i in soup.body.section.find_all('a', href=True)]\n",
    "\n",
    "    haikus_dict = {}\n",
    "    haikus_count = 0\n",
    "\n",
    "    for href_month in archives_href :\n",
    "        # Setting up the link for the request \n",
    "        splitted_link = href_month.split(\"/\")\n",
    "        archive_page_link = splitted_link[0] + \"//\" + splitted_link[2] + \"/\" + splitted_link[3]+ \"/haiku-p\"\n",
    "\n",
    "        if splitted_link[3] == \"Archives\" :\n",
    "            continue\n",
    "\n",
    "        # List to store all the haikus \n",
    "        archive_haikus_list = parsing_archive_page(archive_page_link)\n",
    "        \n",
    "        if len(archive_haikus_list) == 0 : \n",
    "            print(\"Warning : Must do a different parsing for this page !\")\n",
    "        \n",
    "        haikus_dict[splitted_link[3]] = archive_haikus_list\n",
    "\n",
    "        if verbose :\n",
    "            print(\"Paring haikus of : \" + splitted_link[3])\n",
    "            print(f\"Parsed {len(archive_haikus_list)} haikus !\\n\")\n",
    "\n",
    "    return haikus_dict\n",
    "\n",
    "res_parsing_heronsnest = parse_heronsnest_haikus()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now, we will be saving the resulting data in the corresponding csv file !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def heronsnest_to_csv(parsed_haikus) :\n",
    "    cols_names = [\"line_1\", \"line_2\", \"line_3\"]\n",
    "    csv_list = []\n",
    "    for month in parsed_haikus :\n",
    "        csv_list.extend(parsed_haikus[month])\n",
    "    csv = pd.DataFrame(csv_list, columns=cols_names)\n",
    "    csv[\"source\"] = \"herons_nest\"\n",
    "    return csv\n",
    "\n",
    "heronsnest_csv = heronsnest_to_csv(res_parsing_heronsnest)\n",
    "heronsnest_csv.to_csv('heronsnest_csv.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temps libre\n",
    "\n",
    "In this section, we are scrapping data from the Temps Libre website.\n",
    "\n",
    "This step parsing each archives pages of the site and collect the found haikus to store them in a list before saving them in a csv file. \n",
    "\n",
    "This website was found in the list of this GitHub during my research about the subject : [haiku-scraper's GitHub](https://github.com/ytixu/haiku-scraper/tree/master).\n",
    "\n",
    "This is the website used here : [Temps libre](https://www.tempslibres.org/tl/tlphp/dblang.php?lg=e). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### French Haikus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def french_haikus_tempslibre_parsing(verbose=False) :\n",
    "    haikus_list = []\n",
    "    response = requests.get(\"https://www.tempslibres.org/tl/tlphp/dbauteursl.php?lang=fr&lg=e\", headers={\"User-Agent\": \"XY\"})\n",
    "    soup = bs4.BeautifulSoup(response.text)\n",
    "    archives_href = [i['href'] for i in soup.article.find_all('a', href=True)]\n",
    "\n",
    "    for href in archives_href : \n",
    "        link =  \"https://www.tempslibres.org/tl/tlphp/\" + href\n",
    "        href_response = requests.get(link, headers={\"User-Agent\": \"XY\"})\n",
    "        href_soup = bs4.BeautifulSoup(href_response.text)\n",
    "        haikus_href_list = href_soup.body.find_all('p', {\"class\": \"haiku\"})\n",
    "        for haikus_href in haikus_href_list : \n",
    "            haiku_corps = haikus_href.text.split('\\n')\n",
    "            if len(haiku_corps) == 3 :\n",
    "                haiku_corps = [re.sub(r'\\xa0', '', haiku_text) for haiku_text in haiku_corps]\n",
    "                haiku_corps = [re.sub(r'\\r', '', haiku_text) for haiku_text in haiku_corps]\n",
    "                haiku_corps = [re.sub(r'\\‚Äè', '', haiku_text) for haiku_text in haiku_corps]\n",
    "                haiku_corps = list(filter(None, haiku_corps))\n",
    "                if len(haiku_corps) == 2 :\n",
    "                    if verbose :\n",
    "                        print(\"Error with haiku : \")\n",
    "                        print(haiku_corps)\n",
    "                        print(\"SKIPPING\\n\")\n",
    "                    continue\n",
    "                \n",
    "                haikus_list.append(haiku_corps)\n",
    "                if verbose :\n",
    "                    print(haiku_corps)\n",
    "    \n",
    "    return haikus_list\n",
    "\n",
    "res_parsing_tempslibre_french = french_haikus_tempslibre_parsing()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now, we will be saving the resulting data in the corresponding csv file !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def french_tempslibre_to_csv(parsed_haikus) :\n",
    "    cols_names = [\"line_1\", \"line_2\", \"line_3\"]\n",
    "    csv = pd.DataFrame(parsed_haikus, columns=cols_names)\n",
    "    csv[\"source\"] = \"tempslibre\"\n",
    "    return csv\n",
    "\n",
    "french_tempslibre_csv = french_tempslibre_to_csv(res_parsing_tempslibre_french)\n",
    "french_tempslibre_csv.to_csv('french_tempslibre_csv.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### English haikus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_languages_haikus(texts_list) :\n",
    "    fr_list, en_list = [], [] \n",
    "    haiku_list = []\n",
    "    haiku_string = \"\"\n",
    "    # Looping into the texts\n",
    "    for text  in texts_list : \n",
    "        # If we reached an empty line\n",
    "        if text == '' :\n",
    "            lang = detect(haiku_string)\n",
    "            if lang == \"fr\" : \n",
    "                fr_list.append(haiku_list)\n",
    "            elif lang == \"en\" :\n",
    "                en_list.append(haiku_list)\n",
    "            haiku_list = []\n",
    "            haiku_string = \"\"\n",
    "\n",
    "        # Otherwise, store the text\n",
    "        else : \n",
    "            haiku_string += text + '\\n'\n",
    "            haiku_list.append(text)\n",
    "\n",
    "    # At the end, check if there is an haiku and parse it\n",
    "    if len(haiku_list) == 3 :\n",
    "        lang = detect(haiku_string)\n",
    "        if lang == \"fr\" : \n",
    "            fr_list.append(haiku_list)\n",
    "        elif lang == \"en\" :\n",
    "            en_list.append(haiku_list)\n",
    "        haiku_list = []\n",
    "    \n",
    "    return fr_list, en_list\n",
    "\n",
    "def english_haikus_tempslibre_parsing(verbose=False) :\n",
    "    french_haikus_list = []\n",
    "    english_haikus_list = []\n",
    "    response = requests.get(\"https://www.tempslibres.org/tl/tlphp/dbauteursl.php?lang=en&lg=e\", headers={\"User-Agent\": \"XY\"})\n",
    "    soup = bs4.BeautifulSoup(response.text)\n",
    "    archives_href = [i['href'] for i in soup.article.find_all('a', href=True)]\n",
    "\n",
    "    for href in archives_href : \n",
    "        link =  \"https://www.tempslibres.org/tl/tlphp/\" + href\n",
    "        href_response = requests.get(link, headers={\"User-Agent\": \"XY\"})\n",
    "        href_soup = bs4.BeautifulSoup(href_response.text)\n",
    "        haikus_href_list = href_soup.body.find_all('p', {\"class\": \"haiku\"})\n",
    "        for haikus_href in haikus_href_list : \n",
    "            haiku_corps = haikus_href.text.split('\\n')\n",
    "            haiku_corps = [re.sub(r'\\xa0', '', haiku_text) for haiku_text in haiku_corps]\n",
    "            haiku_corps = [re.sub(r'\\r', '', haiku_text) for haiku_text in haiku_corps]\n",
    "            haiku_corps = [re.sub(r'\\(', '', haiku_text) for haiku_text in haiku_corps]\n",
    "            haiku_corps = [re.sub(r'\\)', '', haiku_text) for haiku_text in haiku_corps]\n",
    "\n",
    "            if len(haiku_corps) == 3 or len(haiku_corps) == 7 :\n",
    "                haiku_parsed = parse_languages_haikus(haiku_corps)\n",
    "                if len(haiku_parsed[0]) != 0 :\n",
    "                    french_haikus_list.append(haiku_parsed[0][0])\n",
    "                if len(haiku_parsed[1]) != 0 :\n",
    "                    english_haikus_list.append(haiku_parsed[1][0])\n",
    "            \n",
    "            if verbose :\n",
    "                print(haiku_corps)\n",
    "    \n",
    "    return french_haikus_list, english_haikus_list\n",
    "\n",
    "res_parsing_tempslibre_english = english_haikus_tempslibre_parsing(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now, we will be saving the resulting data in the corresponding csv file !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def english_tempslibre_to_csv(parsed_haikus) :\n",
    "    cols_names = [\"line_1\", \"line_2\", \"line_3\"]\n",
    "    csv_french = pd.DataFrame(parsed_haikus[0], columns=cols_names)\n",
    "    csv_french[\"source\"] = \"tempslibre\"\n",
    "    csv_english = pd.DataFrame(parsed_haikus[1], columns=cols_names)\n",
    "    csv_english[\"source\"] = \"tempslibre\"\n",
    "    return csv_french, csv_english\n",
    "\n",
    "french_2_tempslibre_csv, english_tempslibre_csv = english_tempslibre_to_csv(res_parsing_tempslibre_english)\n",
    "french_2_tempslibre_csv.to_csv('french_2_tempslibre_csv.csv', index=False)\n",
    "english_tempslibre_csv.to_csv('english_tempslibre_csv.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modern Haikus\n",
    "\n",
    "In this section, we are scrapping data from the Temps Libre website.\n",
    "\n",
    "This step parsing each archives pages of the site and collect the found haikus to store them in a list before saving them in a csv file. \n",
    "\n",
    "This website was found in the list of this GitHub during my research about the subject : [haiku-scraper's GitHub](https://github.com/ytixu/haiku-scraper/tree/master).\n",
    "\n",
    "This is the website used here : [Moderns Haikus](http://www.modernhaiku.org/previousissue.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_modern_haikus() :\n",
    "    response = requests.get(\"https://www.modernhaiku.org/previousissue.html\", headers={\"User-Agent\": \"XY\"})\n",
    "    soup = bs4.BeautifulSoup(response.text)\n",
    "\n",
    "    haikus_list = []\n",
    "    href_link_list = soup.table.table.find_all('a', href=True)\n",
    "    for href_link in href_link_list : # Can also be : soup.select('table table td p a')\n",
    "        if not \"MH-Archive\" in href_link['href'] :\n",
    "            href_response = requests.get(\"https://www.modernhaiku.org/\" + href_link['href'].split(\"/\")[0] + \"/haiku.html\", headers={\"User-Agent\": \"XY\"})\n",
    "            href_soup = bs4.BeautifulSoup(href_response.text)\n",
    "\n",
    "            for haiku in href_soup.select('table table table p') :\n",
    "                haiku_corps = haiku.text.split('\\n')\n",
    "                haiku_corps = [haiku_text.strip() for haiku_text in haiku_corps]\n",
    "                haiku_corps = list(filter(None, haiku_corps))\n",
    "\n",
    "                if len(haiku_corps) == 3 :\n",
    "                    haiku_corps = [re.sub(r'\\xa0', '', haiku_text) for haiku_text in haiku_corps]\n",
    "                    haiku_corps = [re.sub(r'\\r', '', haiku_text) for haiku_text in haiku_corps]\n",
    "                    haiku_corps = [re.sub(r'\\(', '', haiku_text) for haiku_text in haiku_corps]\n",
    "                    haiku_corps = [re.sub(r'\\)', '', haiku_text) for haiku_text in haiku_corps]\n",
    "\n",
    "                    haikus_list.append(haiku_corps)\n",
    "\n",
    "    return haikus_list\n",
    "\n",
    "res_parsing_modern_haikus = parse_modern_haikus()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now, we will be saving the resulting data in the corresponding csv file !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modern_haikus_to_csv(parsed_haikus) :\n",
    "    cols_names = [\"line_1\", \"line_2\", \"line_3\"]\n",
    "    csv = pd.DataFrame(parsed_haikus, columns=cols_names)\n",
    "    csv[\"source\"] = \"modern_haikus\"\n",
    "    return csv\n",
    "\n",
    "modern_haikus_csv = modern_haikus_to_csv(res_parsing_modern_haikus)\n",
    "modern_haikus_csv.to_csv('modern_haikus_csv.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
